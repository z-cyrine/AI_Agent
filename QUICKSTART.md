# üöÄ D√©marrage Rapide - Llama 3.3 70B

## En 3 commandes:

```powershell
# 1. Installer Ollama (t√©l√©charger depuis https://ollama.com/download/windows)

# 2. T√©l√©charger Llama 3.3 8B
ollama pull llama3.3:8b

# 3. Tester
python test_llama.py
```

## ‚úÖ C'est tout !

Le projet est d√©j√† configur√© pour utiliser **Llama 3.3 8B** (gratuit, local, offline).

**Pourquoi ce mod√®le ?**
- ‚úÖ **Meilleur pour extraction d'intention** (compr√©hension NL + structured output)
- ‚úÖ **Gratuit** (aucun co√ªt API)
- ‚úÖ **Local** (offline, privacy)
- ‚ùå **PAS Code Llama** (sp√©cialis√© pour g√©n√©ration de code, pas extraction NL)

## üìñ Documentation compl√®te

- [INSTALLATION_LLAMA.md](INSTALLATION_LLAMA.md) - Guide d√©taill√©
- [README.md](README.md) - Architecture du projet
- [GUIDE_DEMARRAGE.md](GUIDE_DEMARRAGE.md) - Tests et utilisation

## üéØ Utilisation

```python
from agents.agent1_interpreter import IntentInterpreterAgent

# Initialisation automatique avec Llama 3.3 70B
agent = IntentInterpreterAgent()

# Extraction d'intention
intent = agent.interpret("je veux d√©ployer une base de donn√©es postgres")
print(intent.json(indent=2))
```

---

**Note**: Pour la plupart des machines, **llama3.3:8b est suffisant** pour l'extraction d'intention. Si vous avez 48+ GB RAM, vous pouvez utiliser `llama3.3:70b` pour une qualit√© l√©g√®rement sup√©rieure.
