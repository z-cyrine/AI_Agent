# Installation Llama 3.3 (GRATUIT, LOCAL)

## ‚úÖ Pourquoi Llama 3.3 8B pour cette t√¢che ?

### Votre t√¢che: Extraction d'intention NL ‚Üí JSON structur√©
- **Llama 3.3 8B**: ‚≠ê **RECOMMAND√â** (meilleur rapport qualit√©/ressources)
  - Excellent pour compr√©hension NL + extraction structur√©e
  - **Gratuit, local, offline, aucun co√ªt API**
  - Privacy: donn√©es restent sur votre machine
  - **L√©ger**: Seulement 8 GB RAM requis
  - Qualit√© largement suffisante pour structured output

- **Llama 3.3 70B**: Alternative si beaucoup de RAM (48+ GB)
  - Qualit√© l√©g√®rement sup√©rieure
  - Mais plus lourd (40 GB t√©l√©chargement, 48 GB RAM)

### ‚ùå Pourquoi PAS Code Llama:
- Code Llama est sp√©cialis√© pour **g√©n√©rer du code** (autocompl√©tion, debugging)
- Pas optimal pour **comprendre** du texte NL et extraire des intentions
- Llama 3.3 > Code Llama pour votre use case

---

## üöÄ Installation (3 √©tapes)

### 1. Installer Ollama
```powershell
# T√©l√©charger: https://ollama.com/download/windows
# Installer le fichier .exe
# Ollama d√©marre automatiquement au d√©marrage de Windows
```

### 2. T√©l√©charger Llama 3.3 8B (RECOMMAND√â)
```powershell
# Ouvrir PowerShell
ollama pull llama3.3:8b

# Alternative si vous avez beaucoup de RAM (48+ GB):
# ollama pull llama3.3:70b
```

**Taille**: ~4 GB pour 8B, ~40 GB pour 70B  
**RAM requise**: 8 GB pour 8B, 48 GB pour 70B

### 3. Configurer le projet
```powershell
# Cr√©er .env
cp .env.example .env

# .env contient d√©j√†:
# LLM_MODEL=llama3.3:8b
# OLLAMA_BASE_URL=http://localhost:11434
```

---

## ‚úÖ Tester l'installation

```powershell
# Test 1: Ollama fonctionne ?
ollama list

# Test 2: Llama r√©pond ?
ollama run llama3.3:8b "Bonjour"

# Test 3: Agent 1 fonctionne ?
python -c "from agents.agent1_interpreter import IntentInterpreterAgent; agent = IntentInterpreterAgent(); print('‚úÖ OK')"
```

---

## üìä Performances attendues

| M√©trique | Llama 3.3 8B ‚≠ê | Llama 3.3 70B |
|----------|-----------------|---------------|
| Qualit√© extraction | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Vitesse (local) | ~0.5-1s/requ√™te | ~2-5s/requ√™te |
| Co√ªt | **GRATUIT** | **GRATUIT** |
| RAM requise | 8 GB | 48 GB |
| T√©l√©chargement | 4 GB | 40 GB |

**Recommandation**: Pour l'extraction d'intention, **llama3.3:8b est largement suffisant**.

---

## üéØ Utilisation

```python
from agents.agent1_interpreter import IntentInterpreterAgent

# Initialisation (utilise automatiquement Llama 3.3 70B)
agent = IntentInterpreterAgent()

# Extraction d'intention
query = "je veux d√©ployer une base de donn√©es postgres avec 16 cores et 32 GB RAM"
intent = agent.interpret(query)

print(intent.json(indent=2))
```

---

## üîß D√©pannage

### Erreur "Connection refused"
```powershell
# V√©rifier qu'Ollama tourne
ollama serve
```

### Erreur "Model not found"
```powershell
# Re-t√©l√©charger le mod√®le
ollama pull llama3.3:8b
```

### M√©moire insuffisante pour 8B
```powershell
# Normalement 8B ne devrait pas poser de probl√®me
# Si encore insuffisant, utiliser la version 1B:
ollama pull llama3.3:1b

# Modifier .env:
# LLM_MODEL=llama3.3:1b
```

---

## üí° Recommandation
8B via Ollama ‚≠ê  
**Avantages**:
- ‚úÖ **GRATUIT** (aucun co√ªt API)
- ‚úÖ **Local** (pas besoin d'internet)
- ‚úÖ **Privacy** (donn√©es sur votre machine)
- ‚úÖ **L√©ger** (8 GB RAM seulement)
- ‚úÖ **Rapide** (~0.5-1s par requ√™te)
- ‚úÖ **Qualit√© largement suffisante** pour extraction d'intention

**Si beaucoup de RAM (48+ GB)**:
- Vous pouvez utiliser `llama3.3:70b` pour qualit√© l√©g√®rement sup√©rieure
- Mais pour structured output, 8B est d√©j√† excellent
- ‚úÖ **Qualit√© excellente** pour extraction d'intention

**Plus besoin de**:
- ‚ùå GPT-4o (co√ªteux: ~$0.01-0.03/requ√™te)
- ‚ùå Claude (co√ªteux: ~$0.015/1K tokens)
- ‚ùå Groq (limites API: 30 req/min)
- ‚ùå Cl√©s API, comptes, facturation

---

## üìö Ressources

- [Ollama](https://ollama.com)
- [Llama 3.3 Model Card](https://ollama.com/library/llama3.3)
- [LangChain Ollama Integration](https://python.langchain.com/docs/integrations/chat/ollama)
