# Configuration API pour Llama 3.3 70B

## Providers Support√©s

### Option 1: Groq (‚≠ê RECOMMAND√â - Le plus rapide)

**Avantages:**
- ‚ö° **Ultra-rapide** (~280 tokens/sec)
- üÜì **Quota gratuit g√©n√©reux** (30 req/min, 6000 tokens/min)
- üéØ **Parfait pour d√©veloppement**

**Configuration:**
1. Cr√©er un compte sur [console.groq.com](https://console.groq.com)
2. G√©n√©rer une cl√© API dans "API Keys"
3. Dans `.env`:
   ```env
   LLM_PROVIDER=groq
   LLM_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxxx
   LLM_MODEL=llama-3.3-70b-versatile
   ```

**Mod√®les disponibles:**
- `llama-3.3-70b-versatile` (recommand√©)
- `llama-3.1-70b-versatile`

---

### Option 2: Together AI (Bonne alternative)

**Avantages:**
- üí∞ **5$ de cr√©dits gratuits** √† l'inscription
- üöÄ **Rapide** (~100-150 tokens/sec)
- üìö **Nombreux mod√®les disponibles**

**Configuration:**
1. Cr√©er un compte sur [api.together.xyz](https://api.together.xyz)
2. R√©cup√©rer votre cl√© API dans "API Keys"
3. Dans `.env`:
   ```env
   LLM_PROVIDER=together
   LLM_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   LLM_MODEL=meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo
   ```

**Mod√®les disponibles:**
- `meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo` (recommand√©)
- `meta-llama/Llama-3.1-70B-Instruct-Turbo`

---

### Option 3: Fireworks AI

**Configuration:**
1. Compte sur [fireworks.ai](https://fireworks.ai)
2. Dans `.env`:
   ```env
   LLM_PROVIDER=fireworks
   LLM_API_KEY=fw_xxxxxxxxxxxxxxxxxxxxx
   LLM_MODEL=accounts/fireworks/models/llama-v3p3-70b-instruct
   ```

---

### Option 4: Replicate

**Configuration:**
1. Compte sur [replicate.com](https://replicate.com)
2. Dans `.env`:
   ```env
   LLM_PROVIDER=replicate
   LLM_API_KEY=r8_xxxxxxxxxxxxxxxxxxxxx
   LLM_MODEL=meta/meta-llama-3.3-70b-instruct
   ```

---

## Installation

```bash
# 1. Installer langchain-openai
pip install langchain-openai

# 2. Remplir votre .env avec la cl√© API
# Voir .env.example pour un template

# 3. Tester
python test_quick.py
```

## Comparaison des Providers

| Provider   | Vitesse       | Quota Gratuit        | Prix (apr√®s quota)     | Recommandation      |
|------------|---------------|----------------------|------------------------|---------------------|
| Groq       | ‚ö°‚ö°‚ö° Ultra   | 30 req/min gratuit   | Pas de pricing public  | üèÜ **D√©veloppement** |
| Together   | ‚ö°‚ö° Rapide   | 5$ gratuits          | ~0.9$/M tokens         | Production          |
| Fireworks  | ‚ö°‚ö° Rapide   | Cr√©dits limit√©s      | ~0.9$/M tokens         | Alternative         |
| Replicate  | ‚ö° Moyen      | Pay-per-use          | ~3-4$/M tokens         | Pas recommand√©      |

## V√©rification de la Configuration

```bash
# Test rapide de connexion
python -c "from agents.agent1_interpreter import IntentInterpreterAgent; agent = IntentInterpreterAgent(); print('‚úÖ Configuration OK')"
```

## D√©pannage

**Erreur: "API key not found"**
- V√©rifiez que `.env` contient bien `LLM_API_KEY=votre_cl√©`
- V√©rifiez qu'il n'y a pas d'espace avant/apr√®s la cl√©

**Erreur: "Rate limit exceeded"**
- Groq: Attendez 1 minute (limite: 30 req/min)
- Together: V√©rifiez vos cr√©dits restants

**Erreur: "Model not found"**
- V√©rifiez le nom du mod√®le selon votre provider
- Voir les tableaux ci-dessus pour les noms exacts

## üéØ Recommandation Finale

**Pour ce projet IBN:**
1. **D√©veloppement/Tests**: Groq (gratuit, rapide, parfait)
2. **Production**: Together AI (cr√©dits gratuits puis ~0.9$/M tokens)

**Note**: Le quota gratuit de Groq (30 req/min) est largement suffisant pour tester et d√©velopper votre syst√®me IBN!
